{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.decomposition import PCA\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(x1, x2):\n",
    "    return np.linalg.norm(x2 - x1)\n",
    "\n",
    "def classes_dict(X, Y):\n",
    "    classes_dict = {}\n",
    "    for x, y in zip(X, Y):\n",
    "        if y not in classes_dict:\n",
    "            classes_dict[y] = []\n",
    "        classes_dict[y].append(x)\n",
    "    return {y : np.array(classes_dict[y]) for y in classes_dict} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parkinsons:\n",
    "    def __init__(self):\n",
    "        self.data = np.loadtxt('parkinsons.csv', delimiter=',', skiprows=1, usecols=range(1, 24))\n",
    "        self.X = np.delete(self.data, 16, axis=1)\n",
    "        self.Y = self.data[:, 16].astype(int)\n",
    "        self.cd = classes_dict(self.X, self.Y)\n",
    "        \n",
    "    def get_random_train_test(self, train_percentage=0.8):\n",
    "        shuffled_cd = {y : np.random.permutation(self.cd[y]) for y in self.cd}\n",
    "        class_splits = {y: int(train_percentage * len(self.cd[y])) for y in shuffled_cd}\n",
    "        train_cd = {y: shuffled_cd[y][:class_splits[y]] for y in shuffled_cd}\n",
    "        test_cd = {y: shuffled_cd[y][class_splits[y]:] for y in shuffled_cd}\n",
    "        train = np.random.permutation([[x, y] for y in train_cd for x in train_cd[y]])\n",
    "        test = np.random.permutation([[x, y] for y in test_cd for x in test_cd[y]])\n",
    "        X_train = np.array([k[0] for k in train])\n",
    "        Y_train = np.array([k[1] for k in train])\n",
    "        X_test = np.array([k[0] for k in test])\n",
    "        Y_test = np.array([k[1] for k in test])\n",
    "        return X_train, Y_train, X_test, Y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tester:\n",
    "\n",
    "    def __init__(self, model, X_test, Y_test):\n",
    "        self.model = model\n",
    "        self.X_test = X_test\n",
    "        self.Y_test = Y_test\n",
    "        self.total = len(X_test)\n",
    "        \n",
    "        self.__test_model()\n",
    "        \n",
    "    def __test_model(self):\n",
    "        self.confusion = np.zeros((2, 2))\n",
    "        \n",
    "        for x, y in zip(self.X_test, self.Y_test):\n",
    "            pred = self.model.predict(x)\n",
    "            self.confusion[y, pred] += 1\n",
    "    \n",
    "    def confusion_matrix(self):\n",
    "        return self.confusion\n",
    "        \n",
    "        \n",
    "class Statistics:\n",
    "    def __init__(self, matrices):\n",
    "        self.m = np.array(matrices)\n",
    "        self.m_sum = self.m.sum(axis=0)\n",
    "    \n",
    "    def mean_accuracy(self):\n",
    "        return self.m_sum.trace() / self.m_sum.sum()\n",
    "    \n",
    "    def specificity(self):\n",
    "        return self.m_sum[0, 0] / self.m_sum.sum(axis=0)[0]\n",
    "    \n",
    "    def sensibility(self):\n",
    "        return self.m_sum[1, 1] / self.m_sum.sum(axis=0)[1]\n",
    "    \n",
    "    def print_all(self):\n",
    "        print(self.m_sum)\n",
    "        print('Mean accuracy: {}'.format(self.mean_accuracy()))\n",
    "        print('Specificity: {}'.format(self.specificity()))\n",
    "        print('Sensibility: {}'.format(self.sensibility()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    \n",
    "    def __init__(self, k=1):\n",
    "        self.k = k\n",
    "        self.values = None\n",
    "    \n",
    "    def train(self, X_train, Y_train):\n",
    "        self.values = list(zip(X_train, Y_train))\n",
    "        \n",
    "    def predict(self, x):\n",
    "        if self.values:\n",
    "            sorted_values = sorted(self.values, key= lambda val : distance(val[0], x))\n",
    "            k_nearest = sorted_values[:self.k]\n",
    "            classes = np.array([k[1] for k in k_nearest])\n",
    "            \n",
    "            return np.bincount(classes).argmax()\n",
    "    \n",
    "        else:\n",
    "            print('Not trained')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DMC:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.clusters = {}\n",
    "        self.centroids = None\n",
    "    \n",
    "    def train(self, X_train, Y_train):\n",
    "        for i in range(len(X_train)):\n",
    "            if Y_train[i] not in self.clusters:\n",
    "                self.clusters[Y_train[i]] = []\n",
    "            self.clusters[Y_train[i]].append(X_train[i])\n",
    "            \n",
    "        self.clusters = {k : np.array(self.clusters[k]) for k in self.clusters}\n",
    "        \n",
    "        self.centroids = {k : np.mean(self.clusters[k], axis=0) for k in self.clusters}\n",
    "        \n",
    "    def predict(self, x):\n",
    "        if self.centroids:\n",
    "            return min(self.centroids, key=lambda c : distance(self.centroids[c], x))\n",
    "    \n",
    "        else:\n",
    "            print('Not trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CQG:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.classes = None\n",
    "        self.centroids = None\n",
    "        self.covariances = None\n",
    "        self.cov_invs = None\n",
    "        self.cov_dets = None\n",
    "        self.a_priori = None\n",
    "        \n",
    "    def __friedman_for_class(self, i, pooled, alpha):\n",
    "        total = sum([len(c) for c in self.classes])\n",
    "        c_len = len(self.classes[i])\n",
    "        return ((1 - alpha) * c_len * self.covariances[i] + alpha * total * pooled) / ((1-alpha) * c_len + alpha * total)\n",
    "        \n",
    "    def regularize_friedman(self, alpha):\n",
    "        total = sum([len(c) for c in self.classes])\n",
    "        pooled = np.sum(np.array([(len(self.classes[i]) / total) * self.covariances[i] for i in range(len(self.classes))]), axis=0)\n",
    "        self.covariances = [self.__friedman_for_class(i, pooled, alpha) for i in range(len(self.classes))]\n",
    "\n",
    "    def __log_gaussian(self, class_n, x):\n",
    "        z = x - self.centroids[class_n]\n",
    "        return -0.5 * (np.matmul(z, np.matmul(self.cov_invs[class_n], z)) + np.log(self.cov_dets[class_n]))\n",
    "\n",
    "    def a_posteriori(self, x):\n",
    "        return np.array([self.__log_gaussian(i, x) for i in range(len(self.classes))])\n",
    "    \n",
    "    def train(self, X_train, Y_train):\n",
    "        cd = classes_dict(X_train, Y_train)\n",
    "        self.classes = [cd[y] for y in sorted(cd)]\n",
    "        \n",
    "        self.centroids = [np.mean(c, axis=0) for c in self.classes]\n",
    "        self.covariances = [np.cov(c, rowvar=False).reshape((len(c[0]), -1)) for c in self.classes]\n",
    "        \n",
    "        if any([np.linalg.matrix_rank(cov_mat) != cov_mat.shape[0] for cov_mat in self.covariances]):\n",
    "            self.regularize_friedman(0.3)\n",
    "            \n",
    "        self.cov_invs = [np.linalg.inv(cov) for cov in self.covariances]\n",
    "        self.cov_dets = [np.linalg.det(cov) for cov in self.covariances]\n",
    "        self.a_priori = np.array([ np.log(len(c)) for c in self.classes ])\n",
    "        \n",
    "    def predict(self, x):\n",
    "        if self.centroids and self.covariances:\n",
    "            probabilities = self.a_priori + self.a_posteriori(x)\n",
    "            return probabilities.argmax()\n",
    "        else:\n",
    "            print('Not trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPCA(X, min_var):\n",
    "    eigs = np.linalg.eigvals(np.dot(X.T, X))\n",
    "    cs = np.cumsum(eigs / eigs.sum())\n",
    "    n_components = len([x for x in cs if x < min_var]) + 1\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(X)\n",
    "    return pca\n",
    "\n",
    "def getLDA(X, Y, min_var):\n",
    "    cd = classes_dict(X, Y)\n",
    "    classes = [cd[y] for y in sorted(cd)]\n",
    "    \n",
    "    m = np.mean(np.array([x for c in classes for x in c]), axis=0)\n",
    "    centroids = [np.mean(c, axis=0) for c in classes]\n",
    "    covariances = [np.cov(c, rowvar=False) for c in classes]\n",
    "    \n",
    "    Sw = sum([len(classes[i]) * covariances[i] for i in range(len(classes))])\n",
    "    Sw_inv = np.linalg.inv(Sw)\n",
    "    n_m = [(c - m).reshape(1, -1) for c in centroids]\n",
    "    Sb = sum([np.dot(n_m[i].T, n_m[i]) for i in range(len(classes))])\n",
    "    \n",
    "    eig = np.linalg.eig(np.dot(Sw_inv, Sb))\n",
    "    eig_vals = eig[0]\n",
    "    cs = np.cumsum(eig_vals / eig_vals.sum())\n",
    "    n_components = len([x for x in cs if x < min_var]) + 1\n",
    "    T = (eig[1][:, n_components]).reshape(-1, 1)\n",
    "    return np.real(T)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.51650723356829\n",
      "21.51650723356829\n",
      "18.202167576742568\n",
      "18.202167576742568\n",
      "36.51178505850744\n",
      "36.51178505850744\n",
      "30.70925743606267\n",
      "30.70925743606267\n",
      "14.510974065563573\n",
      "14.510974065563573\n",
      "10.091496347532484\n",
      "10.091496347532484\n",
      "30.597065562282296\n",
      "30.597065562282296\n",
      "14.566296104785579\n",
      "14.566296104785579\n",
      "59.25781273422945\n",
      "59.25781273422945\n",
      "51.42599710814577\n",
      "51.42599710814577\n",
      "21.93860848698887\n",
      "21.93860848698887\n",
      "49.66549698901508\n",
      "49.66549698901508\n",
      "37.46617272409571\n",
      "37.46617272409571\n",
      "47.20744722214431\n",
      "47.20744722214431\n",
      "20.356494510499033\n",
      "20.356494510499033\n",
      "36.748850382538194\n",
      "36.748850382538194\n",
      "19.154667213952962\n",
      "19.154667213952962\n",
      "23.2079456129768\n",
      "23.2079456129768\n",
      "51.08585379953183\n",
      "51.08585379953183\n",
      "28.384969160000423\n",
      "28.384969160000423\n",
      "49.93954472398059\n",
      "49.93954472398059\n",
      "30.426924052314916\n",
      "30.426924052314916\n",
      "392.2464114533068\n",
      "392.2464114533068\n",
      "227.20655506081675\n",
      "227.20655506081675\n",
      "17.57386661587112\n",
      "17.57386661587112\n",
      "27.261385140405764\n",
      "27.261385140405764\n",
      "14.017473192543218\n",
      "14.017473192543218\n",
      "10.486264534727525\n",
      "10.486264534727525\n",
      "59.62012268480612\n",
      "59.62012268480612\n",
      "29.860972933033736\n",
      "29.860972933033736\n",
      "16.013595608142396\n",
      "16.013595608142396\n",
      "15.192868258085355\n",
      "15.192868258085355\n",
      "17.776161855585542\n",
      "17.776161855585542\n",
      "14.103034860703701\n",
      "14.103034860703701\n",
      "16.586127455525684\n",
      "16.586127455525684\n",
      "20.604669512789314\n",
      "20.604669512789314\n",
      "20.794087562328514\n",
      "20.794087562328514\n",
      "13.530154884686908\n",
      "13.530154884686908\n",
      "122.30013381226854\n",
      "122.30013381226854\n",
      "60.514526494451275\n",
      "60.514526494451275\n",
      "35.05979154097486\n",
      "35.05979154097486\n",
      "22.399029779906414\n",
      "22.399029779906414\n",
      "17.55689200678239\n",
      "17.55689200678239\n",
      "13.502998247690812\n",
      "13.502998247690812\n",
      "50.36056902816563\n",
      "50.36056902816563\n",
      "85.17970911397424\n",
      "85.17970911397424\n",
      "26.86111546059692\n",
      "26.86111546059692\n",
      "13.371983793506388\n",
      "13.371983793506388\n",
      "29.2157234730214\n",
      "29.2157234730214\n",
      "15.791955297974368\n",
      "15.791955297974368\n",
      "83.90252656498751\n",
      "83.90252656498751\n",
      "87.5570287494026\n",
      "87.5570287494026\n",
      "26.497093106473585\n",
      "26.497093106473585\n",
      "27.671064550130268\n",
      "27.671064550130268\n",
      "15.818024372035552\n",
      "15.818024372035552\n",
      "14.3191493516033\n",
      "14.3191493516033\n",
      "119.0061753067148\n",
      "119.0061753067148\n",
      "154.82528866608033\n",
      "154.82528866608033\n",
      "69.26301591635092\n",
      "69.26301591635092\n",
      "38.12159730842268\n",
      "38.12159730842268\n",
      "37.58289522408995\n",
      "37.58289522408995\n",
      "28.090890350117604\n",
      "28.090890350117604\n",
      "44.74352124144457\n",
      "44.74352124144457\n",
      "41.799398053369714\n",
      "41.799398053369714\n",
      "56.07972745353618\n",
      "56.07972745353618\n",
      "34.159795284336724\n",
      "34.159795284336724\n",
      "95.27568961191082\n",
      "95.27568961191082\n",
      "55.89932265102152\n",
      "55.89932265102152\n",
      "27.777572068820064\n",
      "27.777572068820064\n",
      "26.065520122149337\n",
      "26.065520122149337\n",
      "34.02141015048778\n",
      "34.02141015048778\n",
      "25.928206372610475\n",
      "25.928206372610475\n",
      "17.60282861277436\n",
      "17.60282861277436\n",
      "12.18004296305753\n",
      "12.18004296305753\n",
      "19.77151885629138\n",
      "19.77151885629138\n",
      "18.345578442835972\n",
      "18.345578442835972\n",
      "14.62453370918788\n",
      "14.62453370918788\n",
      "11.212759853984608\n",
      "11.212759853984608\n",
      "10.441030200625534\n",
      "10.441030200625534\n",
      "6.158520001958806\n",
      "6.158520001958806\n"
     ]
    }
   ],
   "source": [
    "parkinsons = Parkinsons()\n",
    "matrices = []\n",
    "for i in range(1):\n",
    "    X_train, Y_train, X_test, Y_test = parkinsons.get_random_train_test()\n",
    "    #pca = getPCA(X_train, 0.999)\n",
    "    #X_train = pca.transform(X_train)\n",
    "    #X_test = pca.transform(X_test)\n",
    "    #T = getLDA(X_train, Y_train, 0.999)\n",
    "    #X_train = X_train.dot(T)\n",
    "    #X_test = X_test.dot(T)\n",
    "    \n",
    "    #print(X_train, X_test)\n",
    "    model = CQG()\n",
    "    model.train(X_train, Y_train)\n",
    "    tester = Tester(model, X_test, Y_test)\n",
    "    matrices.append(tester.confusion_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 782.  218.]\n",
      " [ 475. 2525.]]\n",
      "Mean accuracy: 0.82675\n",
      "Specificity: 0.6221161495624503\n",
      "Sensibility: 0.9205249726576741\n"
     ]
    }
   ],
   "source": [
    "statistics = Statistics(matrices)\n",
    "statistics.print_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
