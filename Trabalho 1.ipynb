{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.decomposition import PCA\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(x1, x2):\n",
    "    return np.linalg.norm(x2 - x1)\n",
    "\n",
    "def classes_dict(X, Y):\n",
    "    classes_dict = {}\n",
    "    for x, y in zip(X, Y):\n",
    "        if y not in classes_dict:\n",
    "            classes_dict[y] = []\n",
    "        classes_dict[y].append(x)\n",
    "    return {y : np.array(classes_dict[y]) for y in classes_dict} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parkinsons:\n",
    "    def __init__(self):\n",
    "        self.data = np.loadtxt('parkinsons.csv', delimiter=',', skiprows=1, usecols=range(1, 24))\n",
    "        self.X = np.delete(self.data, 16, axis=1)\n",
    "        self.Y = self.data[:, 16].astype(int)\n",
    "        self.cd = classes_dict(self.X, self.Y)\n",
    "        \n",
    "    def get_random_train_test(self, train_percentage=0.8):\n",
    "        shuffled_cd = {y : np.random.permutation(self.cd[y]) for y in self.cd}\n",
    "        class_splits = {y: int(train_percentage * len(self.cd[y])) for y in shuffled_cd}\n",
    "        train_cd = {y: shuffled_cd[y][:class_splits[y]] for y in shuffled_cd}\n",
    "        test_cd = {y: shuffled_cd[y][class_splits[y]:] for y in shuffled_cd}\n",
    "        train = np.random.permutation([[x, y] for y in train_cd for x in train_cd[y]])\n",
    "        test = np.random.permutation([[x, y] for y in test_cd for x in test_cd[y]])\n",
    "        X_train = np.array([k[0] for k in train])\n",
    "        Y_train = np.array([k[1] for k in train])\n",
    "        X_test = np.array([k[0] for k in test])\n",
    "        Y_test = np.array([k[1] for k in test])\n",
    "        return X_train, Y_train, X_test, Y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tester:\n",
    "\n",
    "    def __init__(self, model, X_test, Y_test):\n",
    "        self.model = model\n",
    "        self.X_test = X_test\n",
    "        self.Y_test = Y_test\n",
    "        self.total = len(X_test)\n",
    "        \n",
    "        self.__test_model()\n",
    "        \n",
    "    def __test_model(self):\n",
    "        self.confusion = np.zeros((2, 2))\n",
    "        \n",
    "        for x, y in zip(self.X_test, self.Y_test):\n",
    "            pred = self.model.predict(x)\n",
    "            self.confusion[y, pred] += 1\n",
    "    \n",
    "    def confusion_matrix(self):\n",
    "        return self.confusion\n",
    "        \n",
    "        \n",
    "class Statistics:\n",
    "    def __init__(self, matrices):\n",
    "        self.m = np.array(matrices)\n",
    "        self.m_sum = self.m.sum(axis=0)\n",
    "    \n",
    "    def mean_accuracy(self):\n",
    "        return self.m_sum.trace() / self.m_sum.sum()\n",
    "    \n",
    "    def specificity(self):\n",
    "        return self.m_sum[0, 0] / self.m_sum.sum(axis=0)[0]\n",
    "    \n",
    "    def sensibility(self):\n",
    "        return self.m_sum[1, 1] / self.m_sum.sum(axis=0)[1]\n",
    "    \n",
    "    def print_all(self):\n",
    "        print(self.m_sum)\n",
    "        print('Mean accuracy: {}'.format(self.mean_accuracy()))\n",
    "        print('Specificity: {}'.format(self.specificity()))\n",
    "        print('Sensibility: {}'.format(self.sensibility()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    \n",
    "    def __init__(self, k=1):\n",
    "        self.k = k\n",
    "        self.values = None\n",
    "    \n",
    "    def train(self, X_train, Y_train):\n",
    "        self.values = list(zip(X_train, Y_train))\n",
    "        \n",
    "    def predict(self, x):\n",
    "        if self.values:\n",
    "            sorted_values = sorted(self.values, key= lambda val : distance(val[0], x))\n",
    "            k_nearest = sorted_values[:self.k]\n",
    "            classes = np.array([k[1] for k in k_nearest])\n",
    "            \n",
    "            return np.bincount(classes).argmax()\n",
    "    \n",
    "        else:\n",
    "            print('Not trained')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DMC:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.clusters = {}\n",
    "        self.centroids = None\n",
    "    \n",
    "    def train(self, X_train, Y_train):\n",
    "        for i in range(len(X_train)):\n",
    "            if Y_train[i] not in self.clusters:\n",
    "                self.clusters[Y_train[i]] = []\n",
    "            self.clusters[Y_train[i]].append(X_train[i])\n",
    "            \n",
    "        self.clusters = {k : np.array(self.clusters[k]) for k in self.clusters}\n",
    "        \n",
    "        self.centroids = {k : np.mean(self.clusters[k], axis=0) for k in self.clusters}\n",
    "        \n",
    "    def predict(self, x):\n",
    "        if self.centroids:\n",
    "            return min(self.centroids, key=lambda c : distance(self.centroids[c], x))\n",
    "    \n",
    "        else:\n",
    "            print('Not trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CQG:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.classes = None\n",
    "        self.centroids = None\n",
    "        self.covariances = None\n",
    "        self.cov_invs = None\n",
    "        self.cov_dets = None\n",
    "        self.a_priori = None\n",
    "        \n",
    "    def __friedman_for_class(self, i, pooled, alpha):\n",
    "        total = sum([len(c) for c in self.classes])\n",
    "        c_len = len(self.classes[i])\n",
    "        return ((1 - alpha) * c_len * self.covariances[i] + alpha * total * pooled) / ((1-alpha) * c_len + alpha * total)\n",
    "        \n",
    "    def regularize_friedman(self, alpha):\n",
    "        total = sum([len(c) for c in self.classes])\n",
    "        pooled = np.sum(np.array([(len(self.classes[i]) / total) * self.covariances[i] for i in range(len(self.classes))]), axis=0)\n",
    "        self.covariances = [self.__friedman_for_class(i, pooled, alpha) for i in range(len(self.classes))]\n",
    "\n",
    "    def __log_gaussian(self, class_n, x):\n",
    "        cov_inv = self.cov_invs[class_n]\n",
    "        cov_det = self.cov_dets[class_n]\n",
    "        z = x - self.centroids[class_n]\n",
    "        return - 0.5 * (np.dot(z, np.dot(cov_inv, z)) + np.log(cov_det))\n",
    "\n",
    "    def a_posteriori(self, x):\n",
    "        return np.array([self.__log_gaussian(i, x) for i in range(len(self.classes))])\n",
    "    \n",
    "    def train(self, X_train, Y_train):\n",
    "        cd = classes_dict(X_train, Y_train)\n",
    "        self.classes = [cd[y] for y in sorted(cd)]\n",
    "        \n",
    "        self.centroids = [np.mean(c, axis=0) for c in self.classes]\n",
    "        self.covariances = [np.cov(c, rowvar=False).reshape((len(c[0]), -1)) for c in self.classes]\n",
    "        \n",
    "        if any([np.linalg.matrix_rank(cov_mat) != cov_mat.shape[0] for cov_mat in self.covariances]):\n",
    "            self.regularize_friedman(0.3)\n",
    "            \n",
    "        self.cov_invs = [np.linalg.inv(cov) for cov in self.covariances]\n",
    "        self.cov_dets = [np.linalg.det(cov) for cov in self.covariances]\n",
    "        self.a_priori = np.array([ np.log(len(c)) for c in self.classes ])\n",
    "        \n",
    "    def predict(self, x):\n",
    "        if self.centroids and self.covariances:\n",
    "            probabilities = self.a_priori + self.a_posteriori(x)\n",
    "            return probabilities.argmax()\n",
    "        else:\n",
    "            print('Not trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPCA(X, min_var):\n",
    "    eigs = np.linalg.eigvals(np.dot(X.T, X))\n",
    "    cs = np.cumsum(eigs / eigs.sum())\n",
    "    n_components = len([x for x in cs if x < min_var]) + 1\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(X)\n",
    "    return pca\n",
    "\n",
    "def getLDA(X, Y, min_var):\n",
    "    cd = classes_dict(X, Y)\n",
    "    classes = [cd[y] for y in sorted(cd)]\n",
    "    \n",
    "    m = np.mean(np.array([x for c in classes for x in c]), axis=0)\n",
    "    centroids = [np.mean(c, axis=0) for c in classes]\n",
    "    covariances = [np.cov(c, rowvar=False) for c in classes]\n",
    "    \n",
    "    Sw = sum([len(classes[i]) * covariances[i] for i in range(len(classes))])\n",
    "    Sw_inv = np.linalg.inv(Sw)\n",
    "    n_m = [(c - m).reshape(1, -1) for c in centroids]\n",
    "    Sb = sum([np.dot(n_m[i].T, n_m[i]) for i in range(len(classes))])\n",
    "    \n",
    "    eig = np.linalg.eig(np.dot(Sw_inv, Sb))\n",
    "    eig_vals = eig[0]\n",
    "    cs = np.cumsum(eig_vals / eig_vals.sum())\n",
    "    n_components = len([x for x in cs if x < min_var]) + 1\n",
    "    T = (eig[1][:, n_components]).reshape(-1, 1)\n",
    "    return np.real(T)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "parkinsons = Parkinsons()\n",
    "matrices = []\n",
    "for i in range(100):\n",
    "    X_train, Y_train, X_test, Y_test = parkinsons.get_random_train_test()\n",
    "    #pca = getPCA(X_train, 0.999)\n",
    "    #X_train = pca.transform(X_train)\n",
    "    #X_test = pca.transform(X_test)\n",
    "    T = getLDA(X_train, Y_train, 0.999)\n",
    "    X_train = X_train.dot(T)\n",
    "    X_test = X_test.dot(T)\n",
    "    \n",
    "    #print(X_train, X_test)\n",
    "    model = CQG()\n",
    "    model.train(X_train, Y_train)\n",
    "    tester = Tester(model, X_test, Y_test)\n",
    "    matrices.append(tester.confusion_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0. 1000.]\n",
      " [   0. 3000.]]\n",
      "Mean accuracy: 0.75\n",
      "Specificity: nan\n",
      "Sensibility: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erick/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "statistics = Statistics(matrices)\n",
    "statistics.print_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
